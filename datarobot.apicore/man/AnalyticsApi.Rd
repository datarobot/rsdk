% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analytics_api.R
\docType{class}
\name{AnalyticsApi}
\alias{AnalyticsApi}
\title{Analytics operations}
\format{
An \code{R6Class} generator object
}
\description{
datarobot.apicore.Analytics
}
\examples{

## ------------------------------------------------
## Method `AnalyticsApi$EventLogsEventsList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsEventsList()
}

## ------------------------------------------------
## Method `AnalyticsApi$EventLogsList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
projectId <- 'projectId_example' # character | The project to select log records for.
userId <- 'userId_example' # character | The user to select log records for.
orgId <- 'orgId_example' # character | The organization to select log records for.
event <- 'event_example' # character | The event type of records.
minTimestamp <- 'minTimestamp_example' # character | The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
maxTimestamp <- 'maxTimestamp_example' # character | The upper bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
offset <- 0 # integer | This many results will be skipped. Defaults to 0.
order <- "desc" # character | The order of the results. Defaults to descending.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc. should be included or not. Defaults to True.
auditReportType <- 'auditReportType_example' # character | Indicates which type of event to return - must be one of ``APP_USAGE`` for application related events (i.e. Project Created, Dataset Uploaded, etc.) or ``ADMIN_USAGE`` for admin related events (i.e. Reset API Token for User, Organization Created, etc.). If not provided, all events will be returned by default.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsList(projectId=projectId, userId=userId, orgId=orgId, event=event, minTimestamp=minTimestamp, maxTimestamp=maxTimestamp, offset=offset, order=order, includeIdentifyingFields=includeIdentifyingFields, auditReportType=auditReportType)
}

## ------------------------------------------------
## Method `AnalyticsApi$EventLogsPredictionUsageList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
minTimestamp <- 'minTimestamp_example' # character | The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
maxTimestamp <- 'maxTimestamp_example' # character | The upper bound for timestamps. Time range should not exceed 24 hours. E.g. '2016-12-13T11:12:13.141516Z'.
projectId <- 'projectId_example' # character | The project to retrieve prediction usage for.
userId <- 'userId_example' # character | The user to retrieve prediction usage for.
order <- "desc" # character | The order of prediction usage rows sorted by ``timestamp``. Defaults to descending.
offset <- 0 # integer | This many results will be skipped. Defaults to 0.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsPredictionUsageList(minTimestamp, maxTimestamp, projectId=projectId, userId=userId, order=order, offset=offset, includeIdentifyingFields=includeIdentifyingFields)
}

## ------------------------------------------------
## Method `AnalyticsApi$EventLogsRetrieve`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
recordId <- 'recordId_example' # character | The id of the audit log.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsRetrieve(recordId, includeIdentifyingFields=includeIdentifyingFields)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsageDataExportsCreate`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
usageDataExport <- UsageDataExport$new() # UsageDataExport | 

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsCreate(usageDataExport=usageDataExport)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsageDataExportsRetrieve`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
artifactId <- 'artifactId_example' # character | The ID of the generated artifact to retrieve.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsRetrieve(artifactId)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsageDataExportsSupportedEventsList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsSupportedEventsList()
}

## ------------------------------------------------
## Method `AnalyticsApi$UsersRateLimitUsageDelete`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The id of the user to reset usage for.
resourceName <- 'resourceName_example' # character | The resource name to reset usage for.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageDelete(userId, resourceName)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsersRateLimitUsageDeleteMany`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The user identifier.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageDeleteMany(userId)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsersRateLimitUsageList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The user identifier.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageList(userId)
}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{apiClient}}{Handles the client-server communication.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-AnalyticsApi-new}{\code{AnalyticsApi$new()}}
\item \href{#method-AnalyticsApi-EventLogsEventsList}{\code{AnalyticsApi$EventLogsEventsList()}}
\item \href{#method-AnalyticsApi-EventLogsList}{\code{AnalyticsApi$EventLogsList()}}
\item \href{#method-AnalyticsApi-EventLogsPredictionUsageList}{\code{AnalyticsApi$EventLogsPredictionUsageList()}}
\item \href{#method-AnalyticsApi-EventLogsRetrieve}{\code{AnalyticsApi$EventLogsRetrieve()}}
\item \href{#method-AnalyticsApi-UsageDataExportsCreate}{\code{AnalyticsApi$UsageDataExportsCreate()}}
\item \href{#method-AnalyticsApi-UsageDataExportsRetrieve}{\code{AnalyticsApi$UsageDataExportsRetrieve()}}
\item \href{#method-AnalyticsApi-UsageDataExportsSupportedEventsList}{\code{AnalyticsApi$UsageDataExportsSupportedEventsList()}}
\item \href{#method-AnalyticsApi-UsersRateLimitUsageDelete}{\code{AnalyticsApi$UsersRateLimitUsageDelete()}}
\item \href{#method-AnalyticsApi-UsersRateLimitUsageDeleteMany}{\code{AnalyticsApi$UsersRateLimitUsageDeleteMany()}}
\item \href{#method-AnalyticsApi-UsersRateLimitUsageList}{\code{AnalyticsApi$UsersRateLimitUsageList()}}
\item \href{#method-AnalyticsApi-clone}{\code{AnalyticsApi$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-new"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-new}{}}}
\subsection{Method \code{new()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$new(apiClient)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{apiClient}}{A configurable \code{ApiClient} instance. If none provided, a new client with default configuration will be created.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-EventLogsEventsList"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-EventLogsEventsList}{}}}
\subsection{Method \code{EventLogsEventsList()}}{
Retrieve all the available events. DEPRECATED API.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$EventLogsEventsList(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Retrieve all the available events. DEPRECATED API.


This method invokes \verb{GET /eventLogs/events/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} A list of events.
\itemize{
}
}
}

\subsection{Returns}{
\link{AuditLogsEventListResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsEventsList()
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-EventLogsList"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-EventLogsList}{}}}
\subsection{Method \code{EventLogsList()}}{
Retrieve one page of audit log records.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$EventLogsList(
  projectId = NULL,
  userId = NULL,
  orgId = NULL,
  event = NULL,
  minTimestamp = NULL,
  maxTimestamp = NULL,
  offset = 0,
  order = "desc",
  includeIdentifyingFields = "true",
  auditReportType = NULL,
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{projectId}}{character. The project to select log records for.}

\item{\code{userId}}{character. The user to select log records for.}

\item{\code{orgId}}{character. The organization to select log records for.}

\item{\code{event}}{Enum < [Dataset Version Deleted, Datasets Permadelete Executed, Bias And Fairness Per Class Calculated, Custom Task Deploy, Dataset Created, Add New Dataset For Predictions, Organization Perma-Deletion Started, Create account, Project Created, Retraining Policy Started, Decision Flow Created, Experiment Container Fetched, Users Perma-Deletion Canceled, Project Created from Project Export File, Workspace Marked As Deleted In AI Catalog, Rate limit user group changed, Detected Data Quality: Lagged Features, Segment Attributes Specified, Recipe Access Revoked from Organization, User Agreement Accepted, Organization created, Group updated, Models Starred, SHAP Predictions Explanations Preview Computed, Experiment Container References Fetched, Finish Autopilot, Automated Application Shared with Group, Geospatial Feature Transform Created, Data Connection Created, Feature Discovery Relationship Quality Assessment Warnings Metrics, Automated Application Shared with Organization, Project Permadelete Executed, Automated Application Access Revoked from the Group, SHAP Predictions Explanations Computed, Custom inference model updated, Update SAML configuration, Workspace scheduled batch processing job updated, Interaction Feature Deployment Created, Custom Model Conversion Files Uploaded, Period accuracy file validation successful, Logout, User Blueprint Added To Repository, Custom task version added, Automodel Deployment Manually Replaced, Successful Login via Google Idp, Challenger Model Deleted, Comment Deleted, Deployment Permanently Erased, Automodel Deployment Created, Retraining Policy Succeeded, Login Fail, API Key Created, FEAR Predict Job Started, Batch prediction job aborted, MLOps Installer Download Request Received, Activated On First Login, Replaced Model, Invitation sent, Download Model Package, Dataset Shared, Project Access Revoked from the Group, Data Stores Permadelete Submitted, Custom task added, Users Perma-Deletion Submitted, Dataset Deleted, Created dataset version from Data Engine workspace, Child Entity Disassociated From Workspace In AI Catalog, Number of bias mitigation jobs on Autopilot stage., Multi-Factor Auth Enable, Recipe Shared with Group, PPS Docker Image Download Request Received, Workspace Tags Modified, Dataset Version Undeleted, Workspace Scheduled Batch Run Started, Notification channel updated, Detected Data Quality: Inliers, Deployment Deleted, Use Case Attachment Removed, Users Perma-Deletion Preview Building Submitted, No predictors are left because of Do-Not-Derive, Users Perma-Deletion Preview Building Completed, Completed Feature Discovery for Primary Dataset, Custom RBAC Access Role Updated, PredictionIntegrationJob Deleted, User Blueprint Tags Modified, Deployment Humility Rule Updated, Global SAML Configuration Updated, Project Cloned, User Blueprint Created, Restart Autopilot, Automated Application Created, Model Insights Job Submitted, External Predictions Configured, Dataset relationship created, Dataset Reloaded, Challenger Models Enabled, Dataset transform created, Multi-Factor Auth Disable, Project Autopilot Configured, Custom RBAC Access Role Created, Feature Discovery Relationship Quality Assessment Inputs Metrics, Download Model Package From Deployment, Automated Application Deleted, Users Perma-Deletion Failed, Deployment Deactivated, First Login After DR Account Migration, Bias and Fairness monitoring settings updated., Activate account, Approve account, Batch prediction job started, Restore Reduced Features, Users Perma-Deletion Preview Building Started, Compliance Doc Previewed, Reset API Token For User, Default value for Do-Not-Derive is changed, aiAPI Portal Login, Dataset featurelist deleted, RuleFit Code Downloaded, Data engine workspace deleted, Download Codegen From Deployment, Custom RBAC Access Role Deleted, Credential Created, Blueprint Search Executed, Job definition created, Decision Flow Test Downloaded, Recipe Access Revoked from User, Bias And Fairness Insights Calculated, Prime Run, Global SAML Configuration Deleted, Challenger Model Promoted, Project Deleted, Notification channel deleted, Deploy Model To Hadoop, PredictionIntegrationJob Created, Download Chart, Deployment Humility Rule Added, Project Access Revoked from the Organization, Data engine query generator created, Data engine workspace created, Approval Workflow Policy Action, Download Deployment Chart, Organization Perma-Deletion Requested, User Blueprint Description Modified, ADLS OAuth User Login Succeeded, Dataset Categories Modified, Update account, User Blueprint Deleted, Retraining Policy Cancelled, Automated Application Access Revoked from the Organization, Change Request Resolved, Automated Document Deleted, Available Forecast Points Computation Job Started, Data Connection Updated, Experiment Container Deleted , Project Options Retrieved, User Agreement Declined, Workspace Modified, Recipe Access Revoked from Group, Deployment prediction export created, Global SAML Configuration Added, Recipe Shared, Custom Model Conversion Failed, Retraining Policy Failed, Download All Charts, Workspace Scheduled Batch Run Failed, Organization Perma-Deletion Unmarked, Challenger Model Created, Credential Deleted, Project Access Revoked from the User, Experiment Container Entity Unlinked, Pipeline downsampling run failed to start., Download Codegen, Do-Not-Derive is used, Organization deleted, Credential Updated, Challenger Models Disabled, Data Sources Permadelete Failed, Approval Workflow Policy Created, Data Stores Permadelete Executed, API Key Deleted, Child Entity Associated With Workspace In AI Catalog, Notification policy updated, Clustering Cluster Names Updated, Experiment Container Entities Fetched, Custom model item added, Dataset refresh job deleted, Prime Downloaded, SHAP Training Predictions Explanations Computed, Project Description Updated, Dataset Name Modified, Segment Analysis Enabled, Compute Cluster Deleted, ADLS OAuth User Login Started, Automated Document Requested, Deployment Added, Detected Data Quality: Disguised Missing Values, Empty Cluster Status Created, Batch prediction job created, Data Stores Permadelete Failed, Select Model Metric, Request Model Insights, User Blueprint Tasks Retrieved, Deployment training data export created, Organization Perma-Deletion Completed, Automated Document Downloaded, Detected Data Quality: Target Leakage, Decision Flow Model Package Created, Detected Data Quality: Imputation Leakage, Detected Data Quality: Leading or Trailing Series, Data Source is created, Workspace Registered In AI Catalog, Custom Model Conversion Succeeded, Dataset Undeleted, Users Perma-Deletion Started, Actuals Uploaded, User Blueprint Deleted In Bulk, Data engine workspace state previewed, Login Succeeded Via Global SAML SSO, Abort Autopilot, Project Shared with Group, Users Perma-Deletion Completed, Project Exported as Project Export File, Managed Image Built, Dataset Download, Data engine workspace updated, Advanced Tuning Requested, Automated Document Created, Retraining Policy Created, Experiment Container Entity Linked, Compute External Insights, Compliance Doc Generated, Change Request Updated, Non Existent Use Case Attachment Removed, Project Permadelete Submitted, Automated Application Shared, ADLS OAuth Token Renewal Succeeded, Change Request Reopened, PredictionIntegrationJob Updated, Users Perma-Deletion Preview Building Canceled, Challenger Insight Generation Started, Successful Decision Flow Test, Detected Data Quality: Missing Images, PredictionIntegrationJob Completed, Detected Data Quality: Missing Documents, User Append Columns Download With Predictions, Workspace Name Modified, Data Connection Tested, User Blueprint Validated, Organizations Perma-Deletion Requested, Automated Application Duplicated, Experiment Container Updated, Calculation of prediction intervals is requested, Detected Data Quality: Quantile Target Sparsity, Change Request Cancelled, Interaction Feature Created, User Blueprint Name Modified, Data engine query generator deleted, Dataset featurelist updated, Dataset refresh job created, Automated Application Domain Prefix Changed, Failed Decision Flow Test, Completed Feature Discovery Secondary Datasets, Add SAML configuration, Dataset featurelist created, Unsupervised Mode Started, Deactivate Account, Workspace Description Modified, Use Case Updated, Approval Workflow Policy Updated, Batch prediction job completed, Dataset Sharing Removed, Users Perma-Deletion Canceling, Datasets Permadelete Submitted, Workspace Scheduled Batch Run Succeeded, SHAP Impact Computed, User Blueprint Updated, Compute Cluster Added, CCM Balancer Terminated, Train Model, Invitation Accepted, Automated Document Previewed, Successful Login using OIDC token exchange, Organization updated, Recipe Shared with Organization, Detected Data Quality: New Series in Recent Data, Created dataset from Data Engine workspace, Geometry Over Geo Computed, Batch Prediction Created from Dataset, Project Permadelete Failed, Detected Data Quality: Multicategorical Invalid Format, Decision Flow Version Deleted, Association ID Set, ADLS OAuth Token Obtained, Dataset relationship updated, Download Predictions, Dataset Materialized, Custom Task Fit, Users Perma-Deletion Preview Building Canceling, Compliance Doc Deleted, Custom inference model added, Deployment prediction warning setting updated, Detected Data Quality: Quantile Target Zero Inflation, Organization Perma-Deletion Failed, Text prediction explanations computed, Project Restored, Empty Catalog Item Created, Deny account, Deployment Humility Setting Updated, Workspace scheduled batch processing job created, Dataset Upload, User Blueprint Retrieved, Project Created from Dataset, Predictions by Forecast Date Settings Updated, Multilabel Labelwise ROC With Missing TPR Or FPR Requested, API Key Updated, Period accuracy insight computed, Project Shared, Base Image Built, Comment Created, User Blueprints Listed, Retraining Policy Deleted, App Config Changed, Use Case Stage Changed, Data Store Config Request Submitted, Data Sources Permadelete Executed, Project Options Updated, Automated Application Access Revoked from the User, Bulk Datasets Tags Appended, Bias and Fairness protected features specified., Deployment Humility Rule Deleted, Successful Login using OIDC flow, Segment Analysis Used, ADLS OAuth Failed, Dataset for predictions with actual value column processed, Detected Data Quality: Inconsistent Gaps, Download Model, Deployment Activated, Login Success Via SAML SSO, Organization Perma-Deletion Marked, Period accuracy file validation failed, Delete SAML configuration, Dataset Tags Modified, Group created, Bias And Fairness Cross Class Calculated, Compliance Doc Downloaded, Datasets Permadelete Failed, CCM CLUSTER Reprovisioned, General Feedback Submitted, Approval Workflow Policy Deleted, Dataset Upload is Completed, Automated Demo Application Created, Detected Data Quality: Outliers, Deployment Humility Rule Submitted, Dataset Column Aliases Modified, Detected Data Quality: Excess Zero, Experiment Container Created, Add Model, Job definition updated, Project Shared with Organization, Detected Data Quality: Target had infrequent negative values, Workspace scheduled batch processing job deleted, Batch prediction job failed, Use Case Attachment Added, Project Target Selected, Notification channel created, CCM Cluster Terminated, Deployment Statistics Reset, Experiment Containers Listed, Start Autopilot, Model Deployment Shared, Group Members Updated, Compute Reason Codes, Project Renamed, Data Sources Permadelete Submitted, Feature Over Geo Computed, Notification policy deleted, Geospatial Primary Location Column Selected, Group deleted, Change Request Review Added, Custom task updated, CCM Resource Group Created, Users Perma-Deletion Preview Building Failed, User Provisioned From JWT, Pipeline downsampling build and run started., PredictionIntegrationJob Failed, Online Conformal PI Calculation Requested, Bulk Datasets Deleted, Automated Application Upgraded, Access request created, Dataset Description Modified, Use Case Created, Login Successful, Decision Flow Version Created, Change Request Created, Data Connection Deleted, SHAP Matrix Computed, Request External Insights, Dataset refresh job updated, CCM Cluster Created, Comment Updated, Target is set as Do-Not-Derive, Documentation Request, Compute Cluster Updated, Notification policy created, Custom Task Prediction Made, Change password] > The event type of records.

[Dataset Version Deleted, Datasets Permadelete Executed, Bias And Fairness Per Class Calculated, Custom Task Deploy, Dataset Created, Add New Dataset For Predictions, Organization Perma-Deletion Started, Create account, Project Created, Retraining Policy Started, Decision Flow Created, Experiment Container Fetched, Users Perma-Deletion Canceled, Project Created from Project Export File, Workspace Marked As Deleted In AI Catalog, Rate limit user group changed, Detected Data Quality: Lagged Features, Segment Attributes Specified, Recipe Access Revoked from Organization, User Agreement Accepted, Organization created, Group updated, Models Starred, SHAP Predictions Explanations Preview Computed, Experiment Container References Fetched, Finish Autopilot, Automated Application Shared with Group, Geospatial Feature Transform Created, Data Connection Created, Feature Discovery Relationship Quality Assessment Warnings Metrics, Automated Application Shared with Organization, Project Permadelete }

\item{\code{minTimestamp}}{character. The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.}

\item{\code{maxTimestamp}}{character. The upper bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.}

\item{\code{offset}}{integer. This many results will be skipped. Defaults to 0.}

\item{\code{order}}{Enum < \link{asc, desc} > The order of the results. Defaults to descending.}

\item{\code{includeIdentifyingFields}}{Enum < \link{false, False, true, True} > Indicates if identifying information like user names, project names, etc. should be included or not. Defaults to True.}

\item{\code{auditReportType}}{Enum < \link{APP_USAGE, ADMIN_USAGE} > Indicates which type of event to return - must be one of ``APP_USAGE`` for application related events (i.e. Project Created, Dataset Uploaded, etc.) or ``ADMIN_USAGE`` for admin related events (i.e. Reset API Token for User, Organization Created, etc.). If not provided, all events will be returned by default.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Retrieve one page of audit log records.


This method invokes \verb{GET /eventLogs/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} A list of audit log records.
\itemize{
}
}
}

\subsection{Returns}{
\link{AuditLogsRetrieveResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
projectId <- 'projectId_example' # character | The project to select log records for.
userId <- 'userId_example' # character | The user to select log records for.
orgId <- 'orgId_example' # character | The organization to select log records for.
event <- 'event_example' # character | The event type of records.
minTimestamp <- 'minTimestamp_example' # character | The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
maxTimestamp <- 'maxTimestamp_example' # character | The upper bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
offset <- 0 # integer | This many results will be skipped. Defaults to 0.
order <- "desc" # character | The order of the results. Defaults to descending.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc. should be included or not. Defaults to True.
auditReportType <- 'auditReportType_example' # character | Indicates which type of event to return - must be one of ``APP_USAGE`` for application related events (i.e. Project Created, Dataset Uploaded, etc.) or ``ADMIN_USAGE`` for admin related events (i.e. Reset API Token for User, Organization Created, etc.). If not provided, all events will be returned by default.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsList(projectId=projectId, userId=userId, orgId=orgId, event=event, minTimestamp=minTimestamp, maxTimestamp=maxTimestamp, offset=offset, order=order, includeIdentifyingFields=includeIdentifyingFields, auditReportType=auditReportType)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-EventLogsPredictionUsageList"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-EventLogsPredictionUsageList}{}}}
\subsection{Method \code{EventLogsPredictionUsageList()}}{
Retrieve prediction usage data.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$EventLogsPredictionUsageList(
  minTimestamp,
  maxTimestamp,
  projectId = NULL,
  userId = NULL,
  order = "desc",
  offset = 0,
  includeIdentifyingFields = "true",
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{minTimestamp}}{character. The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.}

\item{\code{maxTimestamp}}{character. The upper bound for timestamps. Time range should not exceed 24 hours. E.g. '2016-12-13T11:12:13.141516Z'.}

\item{\code{projectId}}{character. The project to retrieve prediction usage for.}

\item{\code{userId}}{character. The user to retrieve prediction usage for.}

\item{\code{order}}{Enum < \link{asc, desc} > The order of prediction usage rows sorted by ``timestamp``. Defaults to descending.}

\item{\code{offset}}{integer. This many results will be skipped. Defaults to 0.}

\item{\code{includeIdentifyingFields}}{Enum < \link{false, False, true, True} > Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Retrieve prediction usage data. `CAN_ACCESS_USER_ACTIVITY` permission is required.


This method invokes \verb{GET /eventLogs/predictionUsage/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} A list of prediction events.
\itemize{
}
}
}

\subsection{Returns}{
\link{PredictionUsageRetrieveResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
minTimestamp <- 'minTimestamp_example' # character | The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
maxTimestamp <- 'maxTimestamp_example' # character | The upper bound for timestamps. Time range should not exceed 24 hours. E.g. '2016-12-13T11:12:13.141516Z'.
projectId <- 'projectId_example' # character | The project to retrieve prediction usage for.
userId <- 'userId_example' # character | The user to retrieve prediction usage for.
order <- "desc" # character | The order of prediction usage rows sorted by ``timestamp``. Defaults to descending.
offset <- 0 # integer | This many results will be skipped. Defaults to 0.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsPredictionUsageList(minTimestamp, maxTimestamp, projectId=projectId, userId=userId, order=order, offset=offset, includeIdentifyingFields=includeIdentifyingFields)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-EventLogsRetrieve"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-EventLogsRetrieve}{}}}
\subsection{Method \code{EventLogsRetrieve()}}{
Get audit record by ID.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$EventLogsRetrieve(
  recordId,
  includeIdentifyingFields = "true",
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{recordId}}{character. The id of the audit log.}

\item{\code{includeIdentifyingFields}}{Enum < \link{false, False, true, True} > Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Get audit record by ID.


This method invokes \verb{GET /eventLogs/\{recordId\}/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} The queried audit record.
\itemize{
}
}
}

\subsection{Returns}{
\link{AuditLogsRetrieveOneResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
recordId <- 'recordId_example' # character | The id of the audit log.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsRetrieve(recordId, includeIdentifyingFields=includeIdentifyingFields)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-UsageDataExportsCreate"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-UsageDataExportsCreate}{}}}
\subsection{Method \code{UsageDataExportsCreate()}}{
Create a customer usage data artifact request. Requires \"CAN_ACCESS_USER_ACTIVITY\" permission.
Produces: NA
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsageDataExportsCreate(usageDataExport = NULL, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{usageDataExport}}{\link{UsageDataExport}.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Create a customer usage data artifact request. Requires \"CAN_ACCESS_USER_ACTIVITY\" permission. Returns async task status URL as a \"Location\" header. Async task status should be polled in order to retrieve artifact id.


This method invokes \verb{POST /usageDataExports/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{202}} Customer usage data artifact creation started. Status can be tracked at Location field in headers.
\itemize{
}
\item \strong{\code{400}} Artifact creation process encountered a problem.
\itemize{
}
}
}

\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
usageDataExport <- UsageDataExport$new() # UsageDataExport | 

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsCreate(usageDataExport=usageDataExport)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-UsageDataExportsRetrieve"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-UsageDataExportsRetrieve}{}}}
\subsection{Method \code{UsageDataExportsRetrieve()}}{
Retrieve a prepared customer usage data artifact.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsageDataExportsRetrieve(artifactId, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{artifactId}}{character. The ID of the generated artifact to retrieve.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Retrieve a prepared customer usage data artifact. Requires \"CAN_ACCESS_USER_ACTIVITY\" permission.


This method invokes \verb{GET /usageDataExports/\{artifactId\}/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{202}} An artifact file in .zip format.
\itemize{
}
\item \strong{\code{400}} Usage data artifact retrieval failed.
\itemize{
}
\item \strong{\code{404}} Requested artifact does not exist.
\itemize{
}
}
}

\subsection{Returns}{
\link{UsageDataRetrieveResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
artifactId <- 'artifactId_example' # character | The ID of the generated artifact to retrieve.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsRetrieve(artifactId)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-UsageDataExportsSupportedEventsList"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-UsageDataExportsSupportedEventsList}{}}}
\subsection{Method \code{UsageDataExportsSupportedEventsList()}}{
Describe supported available audit events with which to filter result data.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsageDataExportsSupportedEventsList(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Describe supported available audit events with which to filter result data.


This method invokes \verb{GET /usageDataExports/supportedEvents/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} A list of supported available audit events.
\itemize{
}
}
}

\subsection{Returns}{
\link{UsageDataEventsListResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsSupportedEventsList()
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-UsersRateLimitUsageDelete"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-UsersRateLimitUsageDelete}{}}}
\subsection{Method \code{UsersRateLimitUsageDelete()}}{
Reset resource usage for the resource
Produces: NA
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsersRateLimitUsageDelete(userId, resourceName, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{userId}}{character. The id of the user to reset usage for.}

\item{\code{resourceName}}{character. The resource name to reset usage for.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Reset rate limit resource usage for a user of a specified resource to zero. This will happen automatically when windows roll over. This route can be used to reset a user's rate limits sooner.


This method invokes \verb{DELETE /users/\{userId\}/rateLimitUsage/\{resourceName\}/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{204}} The usage has been successfully reset to zero.
\itemize{
}
}
}

\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The id of the user to reset usage for.
resourceName <- 'resourceName_example' # character | The resource name to reset usage for.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageDelete(userId, resourceName)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-UsersRateLimitUsageDeleteMany"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-UsersRateLimitUsageDeleteMany}{}}}
\subsection{Method \code{UsersRateLimitUsageDeleteMany()}}{
Reset resource usage for all resources
Produces: NA
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsersRateLimitUsageDeleteMany(userId, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{userId}}{character. The user identifier.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Reset specified user's rate limit resource usage to zero for all resources. When windows roll over, all limits are automatically reset. Use this route to reset usage sooner.


This method invokes \verb{DELETE /users/\{userId\}/rateLimitUsage/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{204}} The usage has been successfully reset to zero.
\itemize{
}
}
}

\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The user identifier.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageDeleteMany(userId)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-UsersRateLimitUsageList"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-UsersRateLimitUsageList}{}}}
\subsection{Method \code{UsersRateLimitUsageList()}}{
List resource usage for a user
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsersRateLimitUsageList(userId, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{userId}}{character. The user identifier.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
List the rate limit resource usage for a user. The usage array returned will have one object corresponding to each rate limit applied to the user.


This method invokes \verb{GET /users/\{userId\}/rateLimitUsage/} in the DataRobot Public API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}}
\itemize{
}
}
}

\subsection{Returns}{
\link{ResourceUsageResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The user identifier.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageList(userId)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AnalyticsApi-clone"></a>}}
\if{latex}{\out{\hypertarget{method-AnalyticsApi-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
