% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analytics_api.R
\docType{class}
\name{AnalyticsApi}
\alias{AnalyticsApi}
\title{Analytics operations}
\format{
An \code{R6Class} generator object
}
\description{
datarobot.apicore.Analytics
}
\examples{

## ------------------------------------------------
## Method `AnalyticsApi$EventLogsEventsList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsEventsList()
}

## ------------------------------------------------
## Method `AnalyticsApi$EventLogsList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
projectId <- 'projectId_example' # character | The project to select log records for.
userId <- 'userId_example' # character | The user to select log records for.
orgId <- 'orgId_example' # character | The organization to select log records for.
event <- 'event_example' # character | The event type of records.
minTimestamp <- 'minTimestamp_example' # character | The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
maxTimestamp <- 'maxTimestamp_example' # character | The upper bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
offset <- 0 # integer | This many results will be skipped. Defaults to 0.
order <- "desc" # character | The order of the results. Defaults to descending.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc. should be included or not. Defaults to True.
auditReportType <- 'auditReportType_example' # character | Indicates which type of event to return - must be one of ``APP_USAGE`` for application related events (i.e. Project Created, Dataset Uploaded, etc.) or ``ADMIN_USAGE`` for admin related events (i.e. Reset API Token for User, Organization Created, etc.). If not provided, all events will be returned by default.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsList(projectId=projectId, userId=userId, orgId=orgId, event=event, minTimestamp=minTimestamp, maxTimestamp=maxTimestamp, offset=offset, order=order, includeIdentifyingFields=includeIdentifyingFields, auditReportType=auditReportType)
}

## ------------------------------------------------
## Method `AnalyticsApi$EventLogsPredictionUsageList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
minTimestamp <- 'minTimestamp_example' # character | The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
maxTimestamp <- 'maxTimestamp_example' # character | The upper bound for timestamps. Time range should not exceed 24 hours. E.g. '2016-12-13T11:12:13.141516Z'.
projectId <- 'projectId_example' # character | The project to retrieve prediction usage for.
userId <- 'userId_example' # character | The user to retrieve prediction usage for.
order <- "desc" # character | The order of prediction usage rows sorted by ``timestamp``. Defaults to descending.
offset <- 0 # integer | This many results will be skipped. Defaults to 0.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsPredictionUsageList(minTimestamp, maxTimestamp, projectId=projectId, userId=userId, order=order, offset=offset, includeIdentifyingFields=includeIdentifyingFields)
}

## ------------------------------------------------
## Method `AnalyticsApi$EventLogsRetrieve`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
recordId <- 'recordId_example' # character | The id of the audit log.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsRetrieve(recordId, includeIdentifyingFields=includeIdentifyingFields)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsageDataExportsCreate`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
usageDataExport <- UsageDataExport$new() # UsageDataExport |

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsCreate(usageDataExport=usageDataExport)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsageDataExportsRetrieve`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
artifactId <- 'artifactId_example' # character | The ID of the generated artifact to retrieve.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsRetrieve(artifactId)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsageDataExportsSupportedEventsList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsSupportedEventsList()
}

## ------------------------------------------------
## Method `AnalyticsApi$UsersRateLimitUsageDelete`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The id of the user to reset usage for.
resourceName <- 'resourceName_example' # character | The resource name to reset usage for.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageDelete(userId, resourceName)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsersRateLimitUsageDeleteMany`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The user identifier.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageDeleteMany(userId)
}

## ------------------------------------------------
## Method `AnalyticsApi$UsersRateLimitUsageList`
## ------------------------------------------------

\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The user identifier.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageList(userId)
}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{apiClient}}{Handles the client-server communication.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{AnalyticsApi$new()}}
\item \href{#method-EventLogsEventsList}{\code{AnalyticsApi$EventLogsEventsList()}}
\item \href{#method-EventLogsList}{\code{AnalyticsApi$EventLogsList()}}
\item \href{#method-EventLogsPredictionUsageList}{\code{AnalyticsApi$EventLogsPredictionUsageList()}}
\item \href{#method-EventLogsRetrieve}{\code{AnalyticsApi$EventLogsRetrieve()}}
\item \href{#method-UsageDataExportsCreate}{\code{AnalyticsApi$UsageDataExportsCreate()}}
\item \href{#method-UsageDataExportsRetrieve}{\code{AnalyticsApi$UsageDataExportsRetrieve()}}
\item \href{#method-UsageDataExportsSupportedEventsList}{\code{AnalyticsApi$UsageDataExportsSupportedEventsList()}}
\item \href{#method-UsersRateLimitUsageDelete}{\code{AnalyticsApi$UsersRateLimitUsageDelete()}}
\item \href{#method-UsersRateLimitUsageDeleteMany}{\code{AnalyticsApi$UsersRateLimitUsageDeleteMany()}}
\item \href{#method-UsersRateLimitUsageList}{\code{AnalyticsApi$UsersRateLimitUsageList()}}
\item \href{#method-clone}{\code{AnalyticsApi$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$new(apiClient)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{apiClient}}{A configurable \code{ApiClient} instance. If none provided, a new client with default configuration will be created.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EventLogsEventsList"></a>}}
\if{latex}{\out{\hypertarget{method-EventLogsEventsList}{}}}
\subsection{Method \code{EventLogsEventsList()}}{
Retrieve all the available events. DEPRECATED API.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$EventLogsEventsList(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Retrieve all the available events. DEPRECATED API.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} A list of events.
\itemize{
}
}
}

\subsection{Returns}{
\link{AuditLogsEventListResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsEventsList()
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EventLogsList"></a>}}
\if{latex}{\out{\hypertarget{method-EventLogsList}{}}}
\subsection{Method \code{EventLogsList()}}{
Retrieve one page of audit log records.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$EventLogsList(
  projectId = NULL,
  userId = NULL,
  orgId = NULL,
  event = NULL,
  minTimestamp = NULL,
  maxTimestamp = NULL,
  offset = 0,
  order = "desc",
  includeIdentifyingFields = "true",
  auditReportType = NULL,
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{projectId}}{character. The project to select log records for.}

\item{\code{userId}}{character. The user to select log records for.}

\item{\code{orgId}}{character. The organization to select log records for.}

\item{\code{event}}{Enum < [Recipe Access Revoked from User, Dataset Name Modified, Automated Document Created, Clustering Cluster Names Updated, Download Model Package, Dataset Created, Deployment Deleted, Update account, Group created, Project Access Revoked from the User, Restart Autopilot, Compliance Doc Downloaded, Invitation sent, Dataset Deleted, Dataset Column Aliases Modified, Non Existent Use Case Attachment Removed, Start Autopilot, Custom inference model updated, Successful Login using OIDC token exchange, Dataset Shared, API Key Created, Models Starred, Delete SAML configuration, Dataset refresh job updated, Custom model item added, Detected Data Quality: Leading or Trailing Series, Pipeline downsampling build and run started., Data Source is created, CCM Cluster Terminated, Train Model, Retraining Policy Cancelled, Compliance Doc Previewed, Workspace Name Modified, Access request created, Users Perma-Deletion Preview Building Submitted, Dataset Download, Dataset Upload is Completed, Credential Deleted, Deployment Humility Setting Updated, Compute Cluster Added, Credential Created, Workspace Modified, Dataset relationship created, Multilabel Labelwise ROC With Missing TPR Or FPR Requested, Workspace Registered In AI Catalog, Recipe Shared, Geospatial Feature Transform Created, Project Shared, Segment Analysis Used, Bias And Fairness Cross Class Calculated, Users Perma-Deletion Preview Building Canceling, Automated Document Downloaded, Recipe Access Revoked from Organization, Project Permadelete Submitted, Notification channel updated, Dataset Tags Modified, Approval Workflow Policy Action, Dataset Reloaded, Workspace scheduled batch processing job created, Automated Document Deleted, Organization deleted, Available Forecast Points Computation Job Started, Dataset featurelist updated, aiAPI Portal Login, Automated Document Requested, Job definition updated, ADLS OAuth User Login Succeeded, Global SAML Configuration Deleted, Batch prediction job completed, Download Predictions, Approval Workflow Policy Updated, Rate limit user group changed, Decision Flow Version Deleted, Dataset featurelist deleted, Feature Over Geo Computed, Use Case Stage Changed, Data Sources Permadelete Executed, Dataset Materialized, User Agreement Accepted, Dataset refresh job created, Change Request Review Added, Challenger Model Deleted, Model Deployment Shared, Download Chart, Project Autopilot Configured, Compliance Doc Deleted, PredictionIntegrationJob Created, Custom RBAC Access Role Deleted, Child Entity Associated With Workspace In AI Catalog, Global SAML Configuration Added, Deployment Permanently Erased, Workspace Scheduled Batch Run Failed, Comment Created, SHAP Impact Computed, Retraining Policy Succeeded, Users Perma-Deletion Canceled, Created dataset version from Data Engine workspace, Segment Analysis Enabled, Custom Model Conversion Failed, Custom task added, Deploy Model To Hadoop, Users Perma-Deletion Preview Building Canceled, Data engine workspace created, User Blueprints Listed, Automated Application Shared with Organization, Deployment Humility Rule Submitted, Data engine workspace updated, Feature Discovery Relationship Quality Assessment Warnings Metrics, Approve account, Custom inference model added, Change Request Reopened, Organization Perma-Deletion Started, Workspace Scheduled Batch Run Succeeded, Project Permadelete Failed, Login Successful, Geospatial Primary Location Column Selected, Decision Flow Created, Download Model Package From Deployment, Online Conformal PI Calculation Requested, Data Connection Updated, Datasets Permadelete Submitted, Dataset Version Deleted, Multi-Factor Auth Disable, Detected Data Quality: New Series in Recent Data, Custom task updated, Datasets Permadelete Executed, Custom Task Prediction Made, User Blueprint Tasks Retrieved, Dataset relationship updated, Interaction Feature Deployment Created, Dataset Description Modified, Dataset featurelist created, Segment Attributes Specified, Prime Downloaded, Automated Application Duplicated, Detected Data Quality: Lagged Features, Batch prediction job created, Custom Task Deploy, Create account, Automated Document Previewed, Automodel Deployment Manually Replaced, Group deleted, Custom RBAC Access Role Created, Project Shared with Organization, Deployment Statistics Reset, Notification channel deleted, Add Model, Unsupervised Mode Started, Data Connection Tested, Deployment Humility Rule Deleted, Predictions by Forecast Date Settings Updated, Add New Dataset For Predictions, ADLS OAuth Token Renewal Succeeded, Data Stores Permadelete Submitted, CCM Cluster Created, Batch prediction job aborted, Number of bias mitigation jobs on Autopilot stage., Recipe Shared with Group, Project Created from Dataset, Custom RBAC Access Role Updated, SHAP Predictions Explanations Computed, Successful Decision Flow Test, Dataset transform created, Users Perma-Deletion Completed, Data engine workspace state previewed, Data Stores Permadelete Executed, ADLS OAuth User Login Started, User Blueprint Name Modified, Deployment Humility Rule Added, Comment Deleted, General Feedback Submitted, Data engine workspace deleted, Decision Flow Model Package Created, Organizations Perma-Deletion Requested, Feature Discovery Relationship Quality Assessment Inputs Metrics, Detected Data Quality: Target Leakage, Update SAML configuration, Change Request Resolved, Reset API Token For User, Deployment Deactivated, PredictionIntegrationJob Deleted, Select Model Metric, Dataset Sharing Removed, Detected Data Quality: Outliers, Geometry Over Geo Computed, Detected Data Quality: Inconsistent Gaps, Approval Workflow Policy Deleted, Project Description Updated, Automodel Deployment Created, Download Codegen From Deployment, Batch prediction job started, Automated Application Deleted, User Blueprint Added To Repository, Deployment prediction warning setting updated, Bias And Fairness Per Class Calculated, Target is set as Do-Not-Derive, Notification channel created, SHAP Matrix Computed, Project Restored, Failed Decision Flow Test, Dataset Upload, First Login After DR Account Migration, Job definition created, Users Perma-Deletion Submitted, Retraining Policy Failed, Project Access Revoked from the Organization, Base Image Built, API Key Deleted, Custom Model Conversion Succeeded, Challenger Models Disabled, Notification policy created, Add SAML configuration, Request External Insights, Notification policy deleted, Abort Autopilot, Organization Perma-Deletion Marked, Empty Catalog Item Created, Data Stores Permadelete Failed, Custom Model Conversion Files Uploaded, Use Case Attachment Removed, Default value for Do-Not-Derive is changed, Recipe Access Revoked from Group, Logout, Batch prediction job failed, Detected Data Quality: Inliers, Project Exported as Project Export File, PredictionIntegrationJob Updated, Data Connection Deleted, Compute External Insights, Organization Perma-Deletion Failed, Restore Reduced Features, Decision Flow Test Downloaded, FEAR Predict Job Started, Automated Demo Application Created, Decision Flow Version Created, Actuals Uploaded, Change password, CCM CLUSTER Reprovisioned, Detected Data Quality: Missing Images, User Append Columns Download With Predictions, User Provisioned From JWT, Project Shared with Group, Deny account, Data engine query generator created, Group Members Updated, Project Renamed, Workspace Scheduled Batch Run Started, Workspace Description Modified, Workspace Tags Modified, PredictionIntegrationJob Completed, Managed Image Built, Data Store Config Request Submitted, PredictionIntegrationJob Failed, Automated Application Shared with Group, Bias And Fairness Insights Calculated, Compliance Doc Generated, Calculation of prediction intervals is requested, Detected Data Quality: Target had infrequent negative values, Users Perma-Deletion Failed, Organization created, Batch Prediction Created from Dataset, Bulk Datasets Deleted, Custom Task Fit, Organization Perma-Deletion Completed, Data engine query generator deleted, Project Created, Bias and Fairness protected features specified., Automated Application Access Revoked from the User, Successful Login using OIDC flow, Pipeline downsampling run failed to start., User Blueprint Tags Modified, Use Case Updated, Do-Not-Derive is used, Compute Cluster Updated, User Blueprint Deleted, Activate account, Users Perma-Deletion Preview Building Completed, Compute Reason Codes, App Config Changed, Detected Data Quality: Excess Zero, Group updated, Detected Data Quality: Multicategorical Invalid Format, Data Connection Created, Use Case Created, Retraining Policy Started, Detected Data Quality: Disguised Missing Values, Completed Feature Discovery for Primary Dataset, Users Perma-Deletion Preview Building Started, Challenger Insight Generation Started, Use Case Attachment Added, Deployment Humility Rule Updated, Download Deployment Chart, Datasets Permadelete Failed, Deployment Added, Text prediction explanations computed, ADLS OAuth Token Obtained, Bias and Fairness monitoring settings updated., Deployment Activated, Bulk Datasets Tags Appended, Multi-Factor Auth Enable, SHAP Predictions Explanations Preview Computed, Invitation Accepted, Created dataset from Data Engine workspace, User Blueprint Description Modified, Prime Run, Finish Autopilot, Recipe Shared with Organization, Organization Perma-Deletion Requested, Change Request Created, Download Codegen, Approval Workflow Policy Created, Dataset for predictions with actual value column processed, Automated Application Created, Successful Login via Google Idp, No predictors are left because of Do-Not-Derive, Retraining Policy Created, Users Perma-Deletion Preview Building Failed, Advanced Tuning Requested, Users Perma-Deletion Canceling, User Blueprint Validated, Detected Data Quality: Quantile Target Sparsity, Notification policy updated, API Key Updated, Dataset Version Undeleted, Project Deleted, Download Model, Challenger Model Promoted, Automated Application Domain Prefix Changed, Custom task version added, Workspace scheduled batch processing job updated, Workspace scheduled batch processing job deleted, External Predictions Configured, Automated Application Shared, PPS Docker Image Download Request Received, Automated Application Access Revoked from the Group, Interaction Feature Created, Dataset Categories Modified, Dataset Undeleted, ADLS OAuth Failed, Documentation Request, User Agreement Declined, Credential Updated, Project Permadelete Executed, Challenger Model Created, Comment Updated, Users Perma-Deletion Started, Compute Cluster Deleted, User Blueprint Retrieved, Login Success Via SAML SSO, Data Sources Permadelete Submitted, Change Request Updated, CCM Resource Group Created, Login Succeeded Via Global SAML SSO, Detected Data Quality: Quantile Target Zero Inflation, Automated Application Access Revoked from the Organization, Global SAML Configuration Updated, Automated Application Upgraded, Download All Charts, Activated On First Login, Workspace Marked As Deleted In AI Catalog, Organization updated, User Blueprint Deleted In Bulk, User Blueprint Updated, Association ID Set, Project Access Revoked from the Group, Retraining Policy Deleted, Login Fail, Project Target Selected, Detected Data Quality: Imputation Leakage, MLOps Installer Download Request Received, Empty Cluster Status Created, Project Cloned, Data Sources Permadelete Failed, Dataset refresh job deleted, Child Entity Disassociated From Workspace In AI Catalog, Replaced Model, Project Created from Project Export File, Organization Perma-Deletion Unmarked, SHAP Training Predictions Explanations Computed, User Blueprint Created, Request Model Insights, Challenger Models Enabled, Change Request Cancelled, Completed Feature Discovery Secondary Datasets, CCM Balancer Terminated] > The event type of records.

[Recipe Access Revoked from User, Dataset Name Modified, Automated Document Created, Clustering Cluster Names Updated, Download Model Package, Dataset Created, Deployment Deleted, Update account, Group created, Project Access Revoked from the User, Restart Autopilot, Compliance Doc Downloaded, Invitation sent, Dataset Deleted, Dataset Column Aliases Modified, Non Existent Use Case Attachment Removed, Start Autopilot, Custom inference model updated, Successful Login using OIDC token exchange, Dataset Shared, API Key Created, Models Starred, Delete SAML configuration, Dataset refresh job updated, Custom model item added, Detected Data Quality: Leading or Trailing Series, Pipeline downsampling build and run started., Data Source is created, CCM Cluster Terminated, Train Model, Retraining Policy Cancelled, Compliance Doc Previewed, Workspace Name Modified, Access request created, Users Perma-Deletion Preview Building Submitted, Dataset Download, Dataset Upload is Completed, Credential Dele}

\item{\code{minTimestamp}}{character. The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.}

\item{\code{maxTimestamp}}{character. The upper bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.}

\item{\code{offset}}{integer. This many results will be skipped. Defaults to 0.}

\item{\code{order}}{Enum < \link{asc, desc} > The order of the results. Defaults to descending.}

\item{\code{includeIdentifyingFields}}{Enum < \link{false, False, true, True} > Indicates if identifying information like user names, project names, etc. should be included or not. Defaults to True.}

\item{\code{auditReportType}}{Enum < \link{APP_USAGE, ADMIN_USAGE} > Indicates which type of event to return - must be one of ``APP_USAGE`` for application related events (i.e. Project Created, Dataset Uploaded, etc.) or ``ADMIN_USAGE`` for admin related events (i.e. Reset API Token for User, Organization Created, etc.). If not provided, all events will be returned by default.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Retrieve one page of audit log records.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} A list of audit log records.
\itemize{
}
}
}

\subsection{Returns}{
\link{AuditLogsRetrieveResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
projectId <- 'projectId_example' # character | The project to select log records for.
userId <- 'userId_example' # character | The user to select log records for.
orgId <- 'orgId_example' # character | The organization to select log records for.
event <- 'event_example' # character | The event type of records.
minTimestamp <- 'minTimestamp_example' # character | The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
maxTimestamp <- 'maxTimestamp_example' # character | The upper bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
offset <- 0 # integer | This many results will be skipped. Defaults to 0.
order <- "desc" # character | The order of the results. Defaults to descending.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc. should be included or not. Defaults to True.
auditReportType <- 'auditReportType_example' # character | Indicates which type of event to return - must be one of ``APP_USAGE`` for application related events (i.e. Project Created, Dataset Uploaded, etc.) or ``ADMIN_USAGE`` for admin related events (i.e. Reset API Token for User, Organization Created, etc.). If not provided, all events will be returned by default.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsList(projectId=projectId, userId=userId, orgId=orgId, event=event, minTimestamp=minTimestamp, maxTimestamp=maxTimestamp, offset=offset, order=order, includeIdentifyingFields=includeIdentifyingFields, auditReportType=auditReportType)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EventLogsPredictionUsageList"></a>}}
\if{latex}{\out{\hypertarget{method-EventLogsPredictionUsageList}{}}}
\subsection{Method \code{EventLogsPredictionUsageList()}}{
Retrieve prediction usage data.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$EventLogsPredictionUsageList(
  minTimestamp,
  maxTimestamp,
  projectId = NULL,
  userId = NULL,
  order = "desc",
  offset = 0,
  includeIdentifyingFields = "true",
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{minTimestamp}}{character. The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.}

\item{\code{maxTimestamp}}{character. The upper bound for timestamps. Time range should not exceed 24 hours. E.g. '2016-12-13T11:12:13.141516Z'.}

\item{\code{projectId}}{character. The project to retrieve prediction usage for.}

\item{\code{userId}}{character. The user to retrieve prediction usage for.}

\item{\code{order}}{Enum < \link{asc, desc} > The order of prediction usage rows sorted by ``timestamp``. Defaults to descending.}

\item{\code{offset}}{integer. This many results will be skipped. Defaults to 0.}

\item{\code{includeIdentifyingFields}}{Enum < \link{false, False, true, True} > Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Retrieve prediction usage data. ``CAN_ACCESS_USER_ACTIVITY`` permission is required.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} A list of prediction events.
\itemize{
}
}
}

\subsection{Returns}{
\link{PredictionUsageRetrieveResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
minTimestamp <- 'minTimestamp_example' # character | The lower bound for timestamps. E.g. '2016-12-13T11:12:13.141516Z'.
maxTimestamp <- 'maxTimestamp_example' # character | The upper bound for timestamps. Time range should not exceed 24 hours. E.g. '2016-12-13T11:12:13.141516Z'.
projectId <- 'projectId_example' # character | The project to retrieve prediction usage for.
userId <- 'userId_example' # character | The user to retrieve prediction usage for.
order <- "desc" # character | The order of prediction usage rows sorted by ``timestamp``. Defaults to descending.
offset <- 0 # integer | This many results will be skipped. Defaults to 0.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsPredictionUsageList(minTimestamp, maxTimestamp, projectId=projectId, userId=userId, order=order, offset=offset, includeIdentifyingFields=includeIdentifyingFields)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EventLogsRetrieve"></a>}}
\if{latex}{\out{\hypertarget{method-EventLogsRetrieve}{}}}
\subsection{Method \code{EventLogsRetrieve()}}{
Get audit record by ID.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$EventLogsRetrieve(
  recordId,
  includeIdentifyingFields = "true",
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{recordId}}{character. The id of the audit log.}

\item{\code{includeIdentifyingFields}}{Enum < \link{false, False, true, True} > Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Get audit record by ID.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} The queried audit record.
\itemize{
}
}
}

\subsection{Returns}{
\link{AuditLogsRetrieveOneResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
recordId <- 'recordId_example' # character | The id of the audit log.
includeIdentifyingFields <- "true" # character | Indicates if identifying information like user names, project names, etc should be included or not. Defaults to True.

api.instance <- AnalyticsApi$new()
result <- api.instance$EventLogsRetrieve(recordId, includeIdentifyingFields=includeIdentifyingFields)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-UsageDataExportsCreate"></a>}}
\if{latex}{\out{\hypertarget{method-UsageDataExportsCreate}{}}}
\subsection{Method \code{UsageDataExportsCreate()}}{
Create a customer usage data artifact request. Requires \"CAN_ACCESS_USER_ACTIVITY\" permission.
Produces: NA
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsageDataExportsCreate(usageDataExport = NULL, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{usageDataExport}}{\link{UsageDataExport}.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Create a customer usage data artifact request. Requires \"CAN_ACCESS_USER_ACTIVITY\" permission. Returns async task status URL as a \"Location\" header. Async task status should be polled in order to retrieve artifact id.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{202}} Customer usage data artifact creation started. Status can be tracked at Location field in headers.
\itemize{
}
\item \strong{\code{400}} Artifact creation process encountered a problem.
\itemize{
}
}
}

\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
usageDataExport <- UsageDataExport$new() # UsageDataExport |

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsCreate(usageDataExport=usageDataExport)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-UsageDataExportsRetrieve"></a>}}
\if{latex}{\out{\hypertarget{method-UsageDataExportsRetrieve}{}}}
\subsection{Method \code{UsageDataExportsRetrieve()}}{
Retrieve a prepared customer usage data artifact.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsageDataExportsRetrieve(artifactId, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{artifactId}}{character. The ID of the generated artifact to retrieve.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Retrieve a prepared customer usage data artifact. Requires \"CAN_ACCESS_USER_ACTIVITY\" permission.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{202}} An artifact file in .zip format.
\itemize{
}
\item \strong{\code{400}} Usage data artifact retrieval failed.
\itemize{
}
\item \strong{\code{404}} Requested artifact does not exist.
\itemize{
}
}
}

\subsection{Returns}{
\link{UsageDataRetrieveResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
artifactId <- 'artifactId_example' # character | The ID of the generated artifact to retrieve.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsRetrieve(artifactId)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-UsageDataExportsSupportedEventsList"></a>}}
\if{latex}{\out{\hypertarget{method-UsageDataExportsSupportedEventsList}{}}}
\subsection{Method \code{UsageDataExportsSupportedEventsList()}}{
Describe supported available audit events with which to filter result data.
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsageDataExportsSupportedEventsList(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Describe supported available audit events with which to filter result data.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}} A list of supported available audit events.
\itemize{
}
}
}

\subsection{Returns}{
\link{UsageDataEventsListResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)

api.instance <- AnalyticsApi$new()
result <- api.instance$UsageDataExportsSupportedEventsList()
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-UsersRateLimitUsageDelete"></a>}}
\if{latex}{\out{\hypertarget{method-UsersRateLimitUsageDelete}{}}}
\subsection{Method \code{UsersRateLimitUsageDelete()}}{
Reset resource usage for the resource
Produces: NA
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsersRateLimitUsageDelete(userId, resourceName, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{userId}}{character. The id of the user to reset usage for.}

\item{\code{resourceName}}{character. The resource name to reset usage for.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Reset rate limit resource usage for a user of a specified resource to zero. This will happen automatically when windows roll over. This route can be used to reset a user's rate limits sooner.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{204}} The usage has been successfully reset to zero.
\itemize{
}
}
}

\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The id of the user to reset usage for.
resourceName <- 'resourceName_example' # character | The resource name to reset usage for.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageDelete(userId, resourceName)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-UsersRateLimitUsageDeleteMany"></a>}}
\if{latex}{\out{\hypertarget{method-UsersRateLimitUsageDeleteMany}{}}}
\subsection{Method \code{UsersRateLimitUsageDeleteMany()}}{
Reset resource usage for all resources
Produces: NA
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsersRateLimitUsageDeleteMany(userId, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{userId}}{character. The user identifier.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Reset specified user's rate limit resource usage to zero for all resources. When windows roll over, all limits are automatically reset. Use this route to reset usage sooner.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{204}} The usage has been successfully reset to zero.
\itemize{
}
}
}

\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The user identifier.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageDeleteMany(userId)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-UsersRateLimitUsageList"></a>}}
\if{latex}{\out{\hypertarget{method-UsersRateLimitUsageList}{}}}
\subsection{Method \code{UsersRateLimitUsageList()}}{
List resource usage for a user
Produces: "application/json"
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$UsersRateLimitUsageList(userId, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{userId}}{character. The user identifier.}

\item{\code{...}}{Optional. Additional named parameters to be passed downward.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
List the rate limit resource usage for a user. The usage array returned will have one object corresponding to each rate limit applied to the user.


Response status codes, messages, and headers:
\itemize{
\item \strong{\code{200}}
\itemize{
}
}
}

\subsection{Returns}{
\link{ResourceUsageResponse}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
library(datarobot.apicore)
userId <- 'userId_example' # character | The user identifier.

api.instance <- AnalyticsApi$new()
result <- api.instance$UsersRateLimitUsageList(userId)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AnalyticsApi$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
