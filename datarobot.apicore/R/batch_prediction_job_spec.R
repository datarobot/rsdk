# Copyright 2021 DataRobot, Inc. and its affiliates.
#
# All rights reserved.
#
# DataRobot, Inc.
#
# This is proprietary source code of DataRobot, Inc. and its
# affiliates.

# Public API
#
# DataRobot's Public facing API
#
# The version of the OpenAPI document: 2.27.0
# Contact: api-maintainer@datarobot.com
# Generated by: https://openapi-generator.tech

#' @docType class
#' @title BatchPredictionJobSpec
#'
#' @description BatchPredictionJobSpec Class
#'
#' @format An \code{R6Class} generator object
#'
#' @field abortOnError  character Should this job abort if too many errors are encountered
#'
#' @field chunkSize  \link{OneOfstringinteger} [optional] Which strategy should be used to determine the chunk size. Can be either a named strategy or a fixed size in bytes.
#'
#' @field columnNamesRemapping  \link{OneOfobjectarray} [optional] Remap (rename or remove columns from) the output from this job
#'
#' @field csvSettings  \link{BatchPredictionJobCSVSettings} 
#'
#' @field deploymentId  character [optional] ID of deployment which is used in job for processing predictions dataset
#'
#' @field disableRowLevelErrorHandling  character Skip row by row error handling
#'
#' @field explanationAlgorithm  character [optional] Which algorithm will be used to calculate prediction explanations
#'
#' @field includePredictionStatus  character Include prediction status column in the output
#'
#' @field includeProbabilities  character Include probabilities for all classes
#'
#' @field includeProbabilitiesClasses  list( character ) Include only probabilities for these specific class names.
#'
#' @field intakeSettings  \link{OneOfAzureIntakeBigQueryIntakeCatalogDSSFileSystemIntakeGCPIntakeHTTPIntakeJDBCIntakeLocalFileIntakeS3IntakeSnowflakeIntakeSynapseIntake} The intake option configured for this job
#'
#' @field maxExplanations  integer Number of explanations requested. Will be ordered by strength.
#'
#' @field numConcurrent  integer [optional] Number of simultaneous requests to run against the prediction instance
#'
#' @field outputSettings  \link{OneOfAzureOutputBigQueryOutputFileSystemOutputGCPOutputHTTPOutputJDBCOutputLocalFileOutputS3OutputSnowflakeOutputSynapseOutputTableau} The output option configured for this job
#'
#' @field passthroughColumns  list( character ) [optional] Pass through columns from the original dataset
#'
#' @field passthroughColumnsSet  character [optional] Pass through all columns from the original dataset
#'
#' @field pinnedModelId  character [optional] Specify a model ID used for scoring
#'
#' @field predictionInstance  \link{BatchPredictionJobPredictionInstance} [optional] 
#'
#' @field predictionWarningEnabled  character [optional] Enable prediction warnings.
#'
#' @field skipDriftTracking  character Skip drift tracking for this job.
#'
#' @field thresholdHigh  numeric [optional] Compute explanations for predictions above this threshold
#'
#' @field thresholdLow  numeric [optional] Compute explanations for predictions below this threshold
#'
#' @field timeseriesSettings  \link{OneOfBatchPredictionJobTimeSeriesSettingsForecastBatchPredictionJobTimeSeriesSettingsHistorical} [optional] Time Series settings included of this job is a Time Series job.
#'
#' @importFrom R6 R6Class
#' @importFrom jsonlite fromJSON toJSON
#' @export
BatchPredictionJobSpec <- R6::R6Class(
  "BatchPredictionJobSpec",
  lock_objects = FALSE,
  private = list(
    # @description A helper function to handle assist with type validation. This function will validate class parameters with definite
    # types assigned to them, as well as handling validation of parameters with anyOf and oneOf types listed. These types
    # can themselves be other R6 objects.
    validateProps = function(abortOnError = NULL, chunkSize = NULL, columnNamesRemapping = NULL, csvSettings = NULL, deploymentId = NULL, disableRowLevelErrorHandling = NULL, explanationAlgorithm = NULL, includePredictionStatus = NULL, includeProbabilities = NULL, includeProbabilitiesClasses = NULL, intakeSettings = NULL, maxExplanations = NULL, numConcurrent = NULL, outputSettings = NULL, passthroughColumns = NULL, passthroughColumnsSet = NULL, pinnedModelId = NULL, predictionInstance = NULL, predictionWarningEnabled = NULL, skipDriftTracking = NULL, thresholdHigh = NULL, thresholdLow = NULL, timeseriesSettings = NULL) {
      if (!is.null(`abortOnError`)) {
        stopifnot(is.logical(`abortOnError`), length(`abortOnError`) == 1)
      }
      if (!is.null(`csvSettings`)) {
        stopifnot(R6::is.R6(`csvSettings`))
      }
      if (!is.null(`disableRowLevelErrorHandling`)) {
        stopifnot(is.logical(`disableRowLevelErrorHandling`), length(`disableRowLevelErrorHandling`) == 1)
      }
      if (!is.null(`includePredictionStatus`)) {
        stopifnot(is.logical(`includePredictionStatus`), length(`includePredictionStatus`) == 1)
      }
      if (!is.null(`includeProbabilities`)) {
        stopifnot(is.logical(`includeProbabilities`), length(`includeProbabilities`) == 1)
      }
      if (!is.null(`includeProbabilitiesClasses`)) {
        stopifnot(is.vector(`includeProbabilitiesClasses`))
      }
      if (!is.null(`intakeSettings`)) {
        .setComplexProperty(typeList = list(AzureIntake, BigQueryIntake, Catalog, DSS, FileSystemIntake, GCPIntake, HTTPIntake, JDBCIntake, LocalFileIntake, S3Intake, SnowflakeIntake, SynapseIntake), propertyData = intakeSettings)
      }
      if (!is.null(`maxExplanations`)) {
        stopifnot(is.numeric(`maxExplanations`), length(`maxExplanations`) == 1)
      }
      if (!is.null(`outputSettings`)) {
        .setComplexProperty(typeList = list(AzureOutput, BigQueryOutput, FileSystemOutput, GCPOutput, HTTPOutput, JDBCOutput, LocalFileOutput, S3Output, SnowflakeOutput, SynapseOutput, Tableau), propertyData = outputSettings)
      }
      if (!is.null(`skipDriftTracking`)) {
        stopifnot(is.logical(`skipDriftTracking`), length(`skipDriftTracking`) == 1)
      }
      if (!is.null(`chunkSize`)) {
        .setPrimitiveProperty(typeList = list("character", "numeric"), propertyData = chunkSize)
      }
      if (!is.null(`columnNamesRemapping`)) {
        .setPrimitiveProperty(typeList = list("character", "array"), propertyData = columnNamesRemapping)
      }
      if (!is.null(`deploymentId`)) {
        stopifnot(is.character(`deploymentId`), length(`deploymentId`) == 1)
      }
      if (!is.null(`explanationAlgorithm`)) {
        stopifnot(is.character(`explanationAlgorithm`), length(`explanationAlgorithm`) == 1)
      }
      if (!is.null(`numConcurrent`)) {
        stopifnot(is.numeric(`numConcurrent`), length(`numConcurrent`) == 1)
      }
      if (!is.null(`passthroughColumns`)) {
        stopifnot(is.vector(`passthroughColumns`))
      }
      if (!is.null(`passthroughColumnsSet`)) {
        stopifnot(is.character(`passthroughColumnsSet`), length(`passthroughColumnsSet`) == 1)
      }
      if (!is.null(`pinnedModelId`)) {
        stopifnot(is.character(`pinnedModelId`), length(`pinnedModelId`) == 1)
      }
      if (!is.null(`predictionInstance`)) {
        stopifnot(R6::is.R6(`predictionInstance`))
      }
      if (!is.null(`predictionWarningEnabled`)) {
        stopifnot(is.logical(`predictionWarningEnabled`), length(`predictionWarningEnabled`) == 1)
      }
      if (!is.null(`thresholdHigh`)) {
        stopifnot(is.numeric(`thresholdHigh`), length(`thresholdHigh`) == 1)
      }
      if (!is.null(`thresholdLow`)) {
        stopifnot(is.numeric(`thresholdLow`), length(`thresholdLow`) == 1)
      }
      if (!is.null(`timeseriesSettings`)) {
        .setComplexProperty(typeList = list(BatchPredictionJobTimeSeriesSettingsForecast, BatchPredictionJobTimeSeriesSettingsHistorical), propertyData = timeseriesSettings)
      }
    }
  ),
  public = list(
    `abortOnError` = NULL,
    `chunkSize` = NULL,
    `columnNamesRemapping` = NULL,
    `csvSettings` = NULL,
    `deploymentId` = NULL,
    `disableRowLevelErrorHandling` = NULL,
    `explanationAlgorithm` = NULL,
    `includePredictionStatus` = NULL,
    `includeProbabilities` = NULL,
    `includeProbabilitiesClasses` = NULL,
    `intakeSettings` = NULL,
    `maxExplanations` = NULL,
    `numConcurrent` = NULL,
    `outputSettings` = NULL,
    `passthroughColumns` = NULL,
    `passthroughColumnsSet` = NULL,
    `pinnedModelId` = NULL,
    `predictionInstance` = NULL,
    `predictionWarningEnabled` = NULL,
    `skipDriftTracking` = NULL,
    `thresholdHigh` = NULL,
    `thresholdLow` = NULL,
    `timeseriesSettings` = NULL,
    #' @description A function used to initialize an instance of this class.
    #' @param abortOnError Should this job abort if too many errors are encountered
    #' @param chunkSize Which strategy should be used to determine the chunk size. Can be either a named strategy or a fixed size in bytes.
    #' @param columnNamesRemapping Remap (rename or remove columns from) the output from this job
    #' @param csvSettings 
    #' @param deploymentId ID of deployment which is used in job for processing predictions dataset
    #' @param disableRowLevelErrorHandling Skip row by row error handling
    #' @param explanationAlgorithm Which algorithm will be used to calculate prediction explanations
    #' @param includePredictionStatus Include prediction status column in the output
    #' @param includeProbabilities Include probabilities for all classes
    #' @param includeProbabilitiesClasses Include only probabilities for these specific class names.
    #' @param intakeSettings The intake option configured for this job
    #' @param maxExplanations Number of explanations requested. Will be ordered by strength.
    #' @param numConcurrent Number of simultaneous requests to run against the prediction instance
    #' @param outputSettings The output option configured for this job
    #' @param passthroughColumns Pass through columns from the original dataset
    #' @param passthroughColumnsSet Pass through all columns from the original dataset
    #' @param pinnedModelId Specify a model ID used for scoring
    #' @param predictionInstance 
    #' @param predictionWarningEnabled Enable prediction warnings.
    #' @param skipDriftTracking Skip drift tracking for this job.
    #' @param thresholdHigh Compute explanations for predictions above this threshold
    #' @param thresholdLow Compute explanations for predictions below this threshold
    #' @param timeseriesSettings Time Series settings included of this job is a Time Series job.
    #' @param validateParams An optional param for auto validating this object's parameters before initialization. Default FALSE.
    #' @param ... Any additional keyword arguments to be passed into this object for initialization.
    initialize = function(
        `abortOnError` = NULL, `csvSettings` = NULL, `disableRowLevelErrorHandling` = NULL, `includePredictionStatus` = NULL, `includeProbabilities` = NULL, `includeProbabilitiesClasses` = NULL, `intakeSettings` = NULL, `maxExplanations` = NULL, `outputSettings` = NULL, `skipDriftTracking` = NULL, `chunkSize` = NULL, `columnNamesRemapping` = NULL, `deploymentId` = NULL, `explanationAlgorithm` = NULL, `numConcurrent` = NULL, `passthroughColumns` = NULL, `passthroughColumnsSet` = NULL, `pinnedModelId` = NULL, `predictionInstance` = NULL, `predictionWarningEnabled` = NULL, `thresholdHigh` = NULL, `thresholdLow` = NULL, `timeseriesSettings` = NULL, validateParams = FALSE, ...
    ) {
      local.optional.var <- list(...)
      requiredProps <- list(`abortOnError`, `csvSettings`, `disableRowLevelErrorHandling`, `includePredictionStatus`, `includeProbabilities`, `includeProbabilitiesClasses`, `intakeSettings`, `maxExplanations`, `outputSettings`, `skipDriftTracking`)
      if (validateParams) {
        lapply(requiredProps, missing)
        private$validateProps(abortOnError, chunkSize, columnNamesRemapping, csvSettings, deploymentId, disableRowLevelErrorHandling, explanationAlgorithm, includePredictionStatus, includeProbabilities, includeProbabilitiesClasses, intakeSettings, maxExplanations, numConcurrent, outputSettings, passthroughColumns, passthroughColumnsSet, pinnedModelId, predictionInstance, predictionWarningEnabled, skipDriftTracking, thresholdHigh, thresholdLow, timeseriesSettings)
      }
      self$`abortOnError` <- `abortOnError`
        self$`chunkSize` <- .setPrimitiveProperty(typeList = list("character", "numeric"), propertyData = chunkSize)
        self$`columnNamesRemapping` <- .setPrimitiveProperty(typeList = list("character", "array"), propertyData = columnNamesRemapping)
      self$`csvSettings` <- `csvSettings`
      self$`deploymentId` <- `deploymentId`
      self$`disableRowLevelErrorHandling` <- `disableRowLevelErrorHandling`
      self$`explanationAlgorithm` <- `explanationAlgorithm`
      self$`includePredictionStatus` <- `includePredictionStatus`
      self$`includeProbabilities` <- `includeProbabilities`
      sapply(`includeProbabilitiesClasses`, function(x) stopifnot(is.character(x)))
        self$`intakeSettings` <- .setComplexProperty(typeList = list(AzureIntake, BigQueryIntake, Catalog, DSS, FileSystemIntake, GCPIntake, HTTPIntake, JDBCIntake, LocalFileIntake, S3Intake, SnowflakeIntake, SynapseIntake), propertyData = intakeSettings)
      self$`maxExplanations` <- `maxExplanations`
      self$`numConcurrent` <- `numConcurrent`
        self$`outputSettings` <- .setComplexProperty(typeList = list(AzureOutput, BigQueryOutput, FileSystemOutput, GCPOutput, HTTPOutput, JDBCOutput, LocalFileOutput, S3Output, SnowflakeOutput, SynapseOutput, Tableau), propertyData = outputSettings)
      sapply(`passthroughColumns`, function(x) stopifnot(is.character(x)))
      self$`passthroughColumnsSet` <- `passthroughColumnsSet`
      self$`pinnedModelId` <- `pinnedModelId`
      self$`predictionInstance` <- `predictionInstance`
      self$`predictionWarningEnabled` <- `predictionWarningEnabled`
      self$`skipDriftTracking` <- `skipDriftTracking`
      self$`thresholdHigh` <- `thresholdHigh`
      self$`thresholdLow` <- `thresholdLow`
        self$`timeseriesSettings` <- .setComplexProperty(typeList = list(BatchPredictionJobTimeSeriesSettingsForecast, BatchPredictionJobTimeSeriesSettingsHistorical), propertyData = timeseriesSettings)
    },
    #' @description A helper function that provides public access to the private validateProps function. This allows users the ability
    #' to programmatically validate objects before sending them to DataRobot.
    validate = function() {
      props <- list(abortOnError = self$`abortOnError`, chunkSize = self$`chunkSize`, columnNamesRemapping = self$`columnNamesRemapping`, csvSettings = self$`csvSettings`, deploymentId = self$`deploymentId`, disableRowLevelErrorHandling = self$`disableRowLevelErrorHandling`, explanationAlgorithm = self$`explanationAlgorithm`, includePredictionStatus = self$`includePredictionStatus`, includeProbabilities = self$`includeProbabilities`, includeProbabilitiesClasses = self$`includeProbabilitiesClasses`, intakeSettings = self$`intakeSettings`, maxExplanations = self$`maxExplanations`, numConcurrent = self$`numConcurrent`, outputSettings = self$`outputSettings`, passthroughColumns = self$`passthroughColumns`, passthroughColumnsSet = self$`passthroughColumnsSet`, pinnedModelId = self$`pinnedModelId`, predictionInstance = self$`predictionInstance`, predictionWarningEnabled = self$`predictionWarningEnabled`, skipDriftTracking = self$`skipDriftTracking`, thresholdHigh = self$`thresholdHigh`, thresholdLow = self$`thresholdLow`, timeseriesSettings = self$`timeseriesSettings`)
      do.call(private$validateProps, props)
    },
    #' @description A helper function that serializes this object into a JSON encoded string.
    toJSON = function() {
      jsoncontent <- c(
        if (!is.null(self$`abortOnError`)) {
        sprintf(
        '"abortOnError":
          %s
                ',
        tolower(self$`abortOnError`)
        )},
        if (!is.null(self$`chunkSize`)) {
        sprintf(
        '"chunkSize":
        %s
        ',
        jsonlite::toJSON(self$`chunkSize`$toJSON(), auto_unbox = TRUE, digits = NA)
        )},
        if (!is.null(self$`columnNamesRemapping`)) {
        sprintf(
        '"columnNamesRemapping":
        %s
        ',
        jsonlite::toJSON(self$`columnNamesRemapping`$toJSON(), auto_unbox = TRUE, digits = NA)
        )},
        if (!is.null(self$`csvSettings`)) {
        sprintf(
        '"csvSettings":
        %s
        ',
        jsonlite::toJSON(self$`csvSettings`$toJSON(), auto_unbox = TRUE, digits = NA)
        )},
        if (!is.null(self$`deploymentId`)) {
        sprintf(
        '"deploymentId":
          "%s"
                ',
        self$`deploymentId`
        )},
        if (!is.null(self$`disableRowLevelErrorHandling`)) {
        sprintf(
        '"disableRowLevelErrorHandling":
          %s
                ',
        tolower(self$`disableRowLevelErrorHandling`)
        )},
        if (!is.null(self$`explanationAlgorithm`)) {
        sprintf(
        '"explanationAlgorithm":
          "%s"
                ',
        self$`explanationAlgorithm`
        )},
        if (!is.null(self$`includePredictionStatus`)) {
        sprintf(
        '"includePredictionStatus":
          %s
                ',
        tolower(self$`includePredictionStatus`)
        )},
        if (!is.null(self$`includeProbabilities`)) {
        sprintf(
        '"includeProbabilities":
          %s
                ',
        tolower(self$`includeProbabilities`)
        )},
        if (!is.null(self$`includeProbabilitiesClasses`)) {
        sprintf(
        '"includeProbabilitiesClasses":
           [%s]
        ',
        paste(unlist(lapply(self$`includeProbabilitiesClasses`, function(x) paste0('"', x, '"'))), collapse=",")
        )},
        if (!is.null(self$`intakeSettings`)) {
        sprintf(
        '"intakeSettings":
        %s
        ',
        jsonlite::toJSON(self$`intakeSettings`$toJSON(), auto_unbox = TRUE, digits = NA)
        )},
        if (!is.null(self$`maxExplanations`)) {
        sprintf(
        '"maxExplanations":
          %d
                ',
        self$`maxExplanations`
        )},
        if (!is.null(self$`numConcurrent`)) {
        sprintf(
        '"numConcurrent":
          %d
                ',
        self$`numConcurrent`
        )},
        if (!is.null(self$`outputSettings`)) {
        sprintf(
        '"outputSettings":
        %s
        ',
        jsonlite::toJSON(self$`outputSettings`$toJSON(), auto_unbox = TRUE, digits = NA)
        )},
        if (!is.null(self$`passthroughColumns`)) {
        sprintf(
        '"passthroughColumns":
           [%s]
        ',
        paste(unlist(lapply(self$`passthroughColumns`, function(x) paste0('"', x, '"'))), collapse=",")
        )},
        if (!is.null(self$`passthroughColumnsSet`)) {
        sprintf(
        '"passthroughColumnsSet":
          "%s"
                ',
        self$`passthroughColumnsSet`
        )},
        if (!is.null(self$`pinnedModelId`)) {
        sprintf(
        '"pinnedModelId":
          "%s"
                ',
        self$`pinnedModelId`
        )},
        if (!is.null(self$`predictionInstance`)) {
        sprintf(
        '"predictionInstance":
        %s
        ',
        jsonlite::toJSON(self$`predictionInstance`$toJSON(), auto_unbox = TRUE, digits = NA)
        )},
        if (!is.null(self$`predictionWarningEnabled`)) {
        sprintf(
        '"predictionWarningEnabled":
          %s
                ',
        tolower(self$`predictionWarningEnabled`)
        )},
        if (!is.null(self$`skipDriftTracking`)) {
        sprintf(
        '"skipDriftTracking":
          %s
                ',
        tolower(self$`skipDriftTracking`)
        )},
        if (!is.null(self$`thresholdHigh`)) {
        sprintf(
        '"thresholdHigh":
          %d
                ',
        self$`thresholdHigh`
        )},
        if (!is.null(self$`thresholdLow`)) {
        sprintf(
        '"thresholdLow":
          %d
                ',
        self$`thresholdLow`
        )},
        if (!is.null(self$`timeseriesSettings`)) {
        sprintf(
        '"timeseriesSettings":
        %s
        ',
        jsonlite::toJSON(self$`timeseriesSettings`$toJSON(), auto_unbox = TRUE, digits = NA)
        )}
      )
      jsoncontent <- paste(jsoncontent, collapse = ",")
      paste("{", jsoncontent, "}", sep = "")
    },
    #' @description A helper function that deserializes a JSON string into an instance of this class.
    #' @param BatchPredictionJobSpecJson A JSON encoded string representation of a class instance.
    #' @param validateParams An optional param for auto validating this object's parameters after deserialization. Default FALSE.
    fromJSON = function(BatchPredictionJobSpecJson, validateParams = FALSE) {
      BatchPredictionJobSpecObject <- jsonlite::fromJSON(BatchPredictionJobSpecJson)
      self$`abortOnError` <- BatchPredictionJobSpecObject$`abortOnError`
      self$`chunkSize` <- .setPrimitiveProperty(typeList = list("character", "numeric"), propertyData = BatchPredictionJobSpecObject$chunkSize)
      self$`columnNamesRemapping` <- .setPrimitiveProperty(typeList = list("character", "array"), propertyData = BatchPredictionJobSpecObject$columnNamesRemapping)
      self$`csvSettings` <- BatchPredictionJobCSVSettings$new()$fromJSON(jsonlite::toJSON(BatchPredictionJobSpecObject$csvSettings, auto_unbox = TRUE, digits = NA, null = "null"))
      self$`deploymentId` <- BatchPredictionJobSpecObject$`deploymentId`
      self$`disableRowLevelErrorHandling` <- BatchPredictionJobSpecObject$`disableRowLevelErrorHandling`
      self$`explanationAlgorithm` <- BatchPredictionJobSpecObject$`explanationAlgorithm`
      self$`includePredictionStatus` <- BatchPredictionJobSpecObject$`includePredictionStatus`
      self$`includeProbabilities` <- BatchPredictionJobSpecObject$`includeProbabilities`
      self$`includeProbabilitiesClasses` <- ApiClient$new()$deserializeObj(BatchPredictionJobSpecObject$`includeProbabilitiesClasses`, "array[character]", loadNamespace("datarobot.apicore"))
      self$`intakeSettings` <- .setComplexProperty(typeList = list(AzureIntake, BigQueryIntake, Catalog, DSS, FileSystemIntake, GCPIntake, HTTPIntake, JDBCIntake, LocalFileIntake, S3Intake, SnowflakeIntake, SynapseIntake), propertyData = BatchPredictionJobSpecObject$intakeSettings)
      self$`maxExplanations` <- BatchPredictionJobSpecObject$`maxExplanations`
      self$`numConcurrent` <- BatchPredictionJobSpecObject$`numConcurrent`
      self$`outputSettings` <- .setComplexProperty(typeList = list(AzureOutput, BigQueryOutput, FileSystemOutput, GCPOutput, HTTPOutput, JDBCOutput, LocalFileOutput, S3Output, SnowflakeOutput, SynapseOutput, Tableau), propertyData = BatchPredictionJobSpecObject$outputSettings)
      self$`passthroughColumns` <- ApiClient$new()$deserializeObj(BatchPredictionJobSpecObject$`passthroughColumns`, "array[character]", loadNamespace("datarobot.apicore"))
      self$`passthroughColumnsSet` <- BatchPredictionJobSpecObject$`passthroughColumnsSet`
      self$`pinnedModelId` <- BatchPredictionJobSpecObject$`pinnedModelId`
      self$`predictionInstance` <- BatchPredictionJobPredictionInstance$new()$fromJSON(jsonlite::toJSON(BatchPredictionJobSpecObject$predictionInstance, auto_unbox = TRUE, digits = NA, null = "null"))
      self$`predictionWarningEnabled` <- BatchPredictionJobSpecObject$`predictionWarningEnabled`
      self$`skipDriftTracking` <- BatchPredictionJobSpecObject$`skipDriftTracking`
      self$`thresholdHigh` <- BatchPredictionJobSpecObject$`thresholdHigh`
      self$`thresholdLow` <- BatchPredictionJobSpecObject$`thresholdLow`
      self$`timeseriesSettings` <- .setComplexProperty(typeList = list(BatchPredictionJobTimeSeriesSettingsForecast, BatchPredictionJobTimeSeriesSettingsHistorical), propertyData = BatchPredictionJobSpecObject$timeseriesSettings)

      if (validateParams) {
        self$validate()
      }

      return(self)
    }
  )
)
